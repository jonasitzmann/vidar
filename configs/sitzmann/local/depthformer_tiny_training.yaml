wrapper:
    recipe: wrapper|default
    max_epochs: 2
#    fp16: True
arch:
    model:
        file: depth/DepthFormerModel
        warp_context: [ -1,1 ]
        match_context: [ -1 ]
        motion_masking: True
        matching_augmentation: False
        freeze_teacher_and_pose: 45
    networks:
        transformer:
            recipe: networks/tiny/transformer|depthformer
        mono_depth:
            recipe: networks/mono_depth_res_net|default
            depth_range: [0.1,100.0]
        multi_depth:
            recipe: networks/tiny/multi_depth_res_net|depthformer
        pose:
            recipe: networks/tiny/pose_net|default
    losses:
        reprojection:
            recipe: losses/reprojection|default
        smoothness:
            recipe: losses/smoothness|default
evaluation:
    depth:
        recipe: evaluation/depth|kitti_resize
optimizers:
    multi_depth:
        recipe: optimizers|adam_20_05
    mono_depth:
        recipe: optimizers|adam_20_05
    pose:
        recipe: optimizers|adam_20_05
    transformer:
        recipe: optimizers|adam_20_05
datasets:
    train:
        path: ['data/datasets/KITTI_tiny']
#        recipe: datasets/kitti_tiny|train_selfsup_mr
        recipe: datasets/kitti_tiny|validation_mr
        context: [ -1,1 ]
        augmentation:
            resize: [128, 416]
        dataloader:
            num_workers: 0
            batch_size: 1
    validation:
        path: ['data/datasets/KITTI_tiny']
        recipe: datasets/kitti_tiny|validation_mr
        context: [ -1,1 ]
        augmentation:
            resize: [128, 416]
        dataloader:
            num_workers: 0
            batch_size: 2
wandb:
    folder: data/wandb     # Where the wandb run is stored
    entity: js0           # Wandb entity
    project: depthformer_local         # Wandb project
#    num_validation_logs: X        # Number of visualization logs
#    tags: [tag1,tag2,...]         # Wandb tags
#    notes: note                   # Wandb notes


